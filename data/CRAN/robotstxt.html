<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>CRAN - Package robotstxt</title>
<link rel="stylesheet" type="text/css" href="../../CRAN_web.css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="citation_title" content="A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler'
Permissions Checker [R package robotstxt version 0.6.2]" />
<meta name="citation_author" content="Peter Meissner" />
<meta name="citation_publication_date.Published" content="2018-07-18" />
<meta name="citation_public_url" content="https://CRAN.R-project.org/package=robotstxt" />
<meta name="DC.identifier" content="https://CRAN.R-project.org/package=robotstxt" />
<meta name="DC.publisher" content="Comprehensive R Archive Network (CRAN)" />
<style type="text/css">
  table td { vertical-align: top; }
</style>
</head>
<body>
<h2>robotstxt: A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler'
Permissions Checker</h2>
<p>Provides functions to download and parse 'robots.txt' files.
        Ultimately the package makes it easy to check if bots
        (spiders, crawler, scrapers, ...) are allowed to access specific
        resources on a domain.</p>
<table summary="Package robotstxt summary">
<tr>
<td>Version:</td>
<td>0.6.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td><a href="../stringr/index.html">stringr</a> (&ge; 1.0.0), <a href="../httr/index.html">httr</a> (&ge; 1.0.0), <a href="../spiderbar/index.html">spiderbar</a> (&ge; 0.2.0), <a href="../future/index.html">future</a> (&ge; 1.6.2), <a href="../future.apply/index.html">future.apply</a> (&ge; 1.0.0), <a href="../magrittr/index.html">magrittr</a>, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td><a href="../knitr/index.html">knitr</a>, <a href="../rmarkdown/index.html">rmarkdown</a>, <a href="../dplyr/index.html">dplyr</a>, <a href="../testthat/index.html">testthat</a>, <a href="../covr/index.html">covr</a></td>
</tr>
<tr>
<td>Published:</td>
<td>2018-07-18</td>
</tr>
<tr>
<td>Author:</td>
<td>Peter Meissner [aut, cre],
  Oliver Keys [ctb],
  Rich Fitz John [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Peter Meissner  &#x3c;&#x72;&#x65;&#x74;&#x65;&#x70;&#x2e;&#x6d;&#x65;&#x69;&#x73;&#x73;&#x6e;&#x65;&#x72;&#x20;&#x61;&#x74;&#x20;&#x67;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;&#x3e;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ropensci/robotstxt/issues">https://github.com/ropensci/robotstxt/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="../../licenses/MIT">MIT</a> + file <a href="LICENSE">LICENSE</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ropensci/robotstxt">https://github.com/ropensci/robotstxt</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Materials:</td>
<td><a href="readme/README.html">README</a> <a href="news/news.html">NEWS</a> </td>
</tr>
<tr>
<td>In&nbsp;views:</td>
<td><a href="../../views/WebTechnologies.html">WebTechnologies</a></td>
</tr>
<tr>
<td>CRAN&nbsp;checks:</td>
<td><a href="../../checks/check_results_robotstxt.html">robotstxt results</a></td>
</tr>
</table>
<h4>Downloads:</h4>
<table summary="Package robotstxt downloads">
<tr>
<td> Reference&nbsp;manual: </td>
<td> <a href="robotstxt.pdf"> robotstxt.pdf </a> </td>
</tr>
<tr>
<td>Vignettes:</td>
<td>
<a href="vignettes/using_robotstxt.html">using_robotstxt</a><br/>
</td>
</tr>
<tr>
<td> Package&nbsp;source: </td>
<td> <a href="../../../src/contrib/robotstxt_0.6.2.tar.gz"> robotstxt_0.6.2.tar.gz </a> </td>
</tr>
<tr>
<td> Windows&nbsp;binaries: </td>
<td> r-devel: <a href="../../../bin/windows/contrib/3.6/robotstxt_0.6.2.zip">robotstxt_0.6.2.zip</a>, r-release: <a href="../../../bin/windows/contrib/3.5/robotstxt_0.6.2.zip">robotstxt_0.6.2.zip</a>, r-oldrel: <a href="../../../bin/windows/contrib/3.4/robotstxt_0.6.2.zip">robotstxt_0.6.2.zip</a> </td>
</tr>
<tr>
<td> OS&nbsp;X&nbsp;binaries: </td>
<td> r-release: <a href="../../../bin/macosx/el-capitan/contrib/3.5/robotstxt_0.6.2.tgz">robotstxt_0.6.2.tgz</a>, r-oldrel: <a href="../../../bin/macosx/el-capitan/contrib/3.4/robotstxt_0.6.2.tgz">robotstxt_0.6.2.tgz</a> </td>
</tr>
<tr>
<td> Old&nbsp;sources: </td>
<td> <a href="https://CRAN.R-project.org/src/contrib/Archive/robotstxt"> robotstxt archive </a> </td>
</tr>
</table>
<h4>Reverse dependencies:</h4>
<table summary="Package robotstxt reverse dependencies">
<tr>
<td>Reverse&nbsp;suggests:</td>
<td><a href="../rzeit2/index.html">rzeit2</a>, <a href="../spiderbar/index.html">spiderbar</a></td>
</tr>
</table>
<h4>Linking:</h4>
<p>Please use the canonical form
<a href="https://CRAN.R-project.org/package=robotstxt"><samp>https://CRAN.R-project.org/package=robotstxt</samp></a>
to link to this page.</p>
</body>
</html>
